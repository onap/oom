{
  "comments": [
    {
      "key": {
        "uuid": "5a93c517_54cf2056",
        "filename": "kubernetes/appc/templates/pv.yaml",
        "patchSetId": 1
      },
      "lineNbr": 16,
      "author": {
        "id": 659
      },
      "writtenOn": "2018-04-25T15:04:52Z",
      "side": 1,
      "message": "I don\u0027t see a definition for .Values.persistence.existingClaim.  If one were to define this and set it to true, what would the expected behaviour/use case be?  Is this for installing on top of an old deployment to (re-)use the MD-SAL DB?",
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_55c5fe59",
        "filename": "kubernetes/appc/templates/pv.yaml",
        "patchSetId": 1
      },
      "lineNbr": 38,
      "author": {
        "id": 659
      },
      "writtenOn": "2018-04-16T13:20:30Z",
      "side": 1,
      "message": "There\u0027s an outstanding bug raised to make the default APP-C deployment have just a single instance (OOM-926).  Clustering will be enabled in a production version of the top-level values.yaml.\nI suggest wrapping the creation of the 2nd and 3rd PVs in a condition that checks against the replicaCount for APP-C.  That way we\u0027ll avoid any unnecessary PVs.",
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_74e49cdf",
        "filename": "kubernetes/appc/templates/statefulset.yaml",
        "patchSetId": 1
      },
      "lineNbr": 187,
      "author": {
        "id": 131
      },
      "writtenOn": "2018-04-25T15:02:46Z",
      "side": 1,
      "message": "Should this be commented out?  I am confused by commenting out this piece but keeping the persistence.enabled values.yaml entry.",
      "range": {
        "startLine": 187,
        "startChar": 0,
        "endLine": 187,
        "endChar": 41
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_9a7f25e0",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 116,
      "author": {
        "id": 1733
      },
      "writtenOn": "2018-04-19T11:03:17Z",
      "side": 1,
      "message": "I see the MDSAL data in a different parent directory structure than the APPC database data. Is there a reason to segment the data in different parent directory (/dockerdata-nfs/appc vs. /dockerdata-nfs/onap/appc) structures?\n\n$ ls -la /dockerdata-nfs/appc/appc\ntotal 20\ndrwxr-xr-x 5 root root 4096 Apr 19 06:55 .\ndrwxr-xr-x 3 root root 4096 Apr 19 06:55 ..\ndrwxr-xr-x 2 root root 4096 Apr 19 06:55 mdsal0\ndrwxr-xr-x 2 root root 4096 Apr 19 06:55 mdsal1\ndrwxr-xr-x 2 root root 4096 Apr 19 06:55 mdsal2\n\n$ ls -la /dockerdata-nfs/onap/appc/data\ntotal 1921904\ndrwxr-xr-x 7  999 docker       4096 Apr 19 06:55 .\ndrwxr-xr-x 3 root root         4096 Apr 16 17:05 ..\n-rw-r----- 1  999 docker        177 Apr 16 17:05 appc-appc-db-0-bin.000001\n-rw-r----- 1  999 docker    3038723 Apr 16 17:05 appc-appc-db-0-bin.000002\n-rw-r----- 1  999 docker 1073766970 Apr 17 03:41 appc-appc-db-0-bin.000003\n-rw-r----- 1  999 docker  597030280 Apr 17 08:57 appc-appc-db-0-bin.000004\n-rw-r----- 1  999 docker        177 Apr 17 10:44 appc-appc-db-0-bin.000005\n-rw-r----- 1  999 docker   16901635 Apr 17 11:07 appc-appc-db-0-bin.000006\n-rw-r----- 1  999 docker   18978620 Apr 17 11:36 appc-appc-db-0-bin.000007\n-rw-r----- 1  999 docker   30357749 Apr 17 16:45 appc-appc-db-0-bin.000008\n-rw-r----- 1  999 docker   17904905 Apr 17 19:36 appc-appc-db-0-bin.000009\n-rw-r----- 1  999 docker   16903179 Apr 19 06:51 appc-appc-db-0-bin.000010\n-rw-r----- 1  999 docker      66640 Apr 19 06:58 appc-appc-db-0-bin.000011\n-rw-r----- 1  999 docker        308 Apr 19 06:55 appc-appc-db-0-bin.index\ndrwxr-x--- 2  999 docker       4096 Apr 16 18:01 appcctl\n-rw-r----- 1  999 docker         56 Apr 16 17:05 auto.cnf\n-rw------- 1  999 docker       1675 Apr 16 17:05 ca-key.pem\n-rw-r--r-- 1  999 docker       1107 Apr 16 17:05 ca.pem\n-rw-r--r-- 1  999 docker       1107 Apr 16 17:05 client-cert.pem\n-rw------- 1  999 docker       1679 Apr 16 17:05 client-key.pem\n-rw-r----- 1  999 docker        965 Apr 19 06:51 ib_buffer_pool\n-rw-r----- 1  999 docker   79691776 Apr 19 06:58 ibdata1\n-rw-r----- 1  999 docker   50331648 Apr 19 06:58 ib_logfile0\n-rw-r----- 1  999 docker   50331648 Apr 19 06:58 ib_logfile1\n-rw-r----- 1  999 docker   12582912 Apr 19 06:56 ibtmp1\ndrwxr-x--- 2  999 docker       4096 Apr 16 17:05 mysql\ndrwxr-x--- 2  999 docker       4096 Apr 16 17:05 performance_schema\n-rw------- 1  999 docker       1679 Apr 16 17:05 private_key.pem\n-rw-r--r-- 1  999 docker        451 Apr 16 17:05 public_key.pem\ndrwxr-x--- 2  999 docker       4096 Apr 16 18:07 sdnctl\n-rw-r--r-- 1  999 docker       1107 Apr 16 17:05 server-cert.pem\n-rw------- 1  999 docker       1675 Apr 16 17:05 server-key.pem\ndrwxr-x--- 2  999 docker      12288 Apr 16 17:05 sys\n\n\n$ kubectl get pv\nNAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                             STORAGECLASS        REASON    AGE\nappc-appc-data0     1Gi        RWO            Retain           Bound     onap/appc-appc-data-appc-appc-2   appc-appc-data                12s\nappc-appc-data1     1Gi        RWO            Retain           Bound     onap/appc-appc-data-appc-appc-0   appc-appc-data                12s\nappc-appc-data2     1Gi        RWO            Retain           Bound     onap/appc-appc-data-appc-appc-1   appc-appc-data                12s\nappc-appc-db-data   1Gi        RWX            Retain           Bound     onap/appc-appc-db-data            appc-appc-db-data             12s",
      "range": {
        "startLine": 116,
        "startChar": 1,
        "endLine": 116,
        "endChar": 26
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_5aeafd74",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 116,
      "author": {
        "id": 659
      },
      "writtenOn": "2018-04-19T14:26:09Z",
      "side": 1,
      "message": "The folders do need to be separate, as they\u0027re owned by PVs from different charts, but should share a common parent.",
      "parentUuid": "5a93c517_9a7f25e0",
      "range": {
        "startLine": 116,
        "startChar": 1,
        "endLine": 116,
        "endChar": 26
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_a4b51b33",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 116,
      "author": {
        "id": 1733
      },
      "writtenOn": "2018-04-23T15:17:16Z",
      "side": 1,
      "message": "I see a problem when I uninstall APPC with these new volumes. They do not clean up properly and I can not install APPC again.\n\n$ helm install local/onap -n appc --namespace onap\nError: release appc failed: object is being deleted: persistentvolumes \"appc-appc-data1\" already exists\nubuntu@k8s-master:~/oom/kubernetes$ helm del --purge appc\nrelease \"appc\" deleted\nubuntu@k8s-master:~/oom/kubernetes$ kubectl get pvc\nNo resources found.\nubuntu@k8s-master:~/oom/kubernetes$ kubectl get pv\nNAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                             STORAGECLASS     REASON    AGE\nappc-appc-data0   1Gi        RWO            Retain           Terminating   onap/appc-appc-data-appc-appc-2   appc-appc-data             4d\nappc-appc-data1   1Gi        RWO            Retain           Terminating   onap/appc-appc-data-appc-appc-0   appc-appc-data             4d\nappc-appc-data2   1Gi        RWO            Retain           Terminating   onap/appc-appc-data-appc-appc-1   appc-appc-data             4d",
      "parentUuid": "5a93c517_5aeafd74",
      "range": {
        "startLine": 116,
        "startChar": 1,
        "endLine": 116,
        "endChar": 26
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_d4d6d04e",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 116,
      "author": {
        "id": 497
      },
      "writtenOn": "2018-04-25T13:43:49Z",
      "side": 1,
      "message": "Aaron, you can see in the error that the object is being deleted. You should let k8s finish cleanup of the deleted PVs and then try to redeploy appc.\nI deployed appc, purged it and deployed again:\nroot@borislav-new-rancher:~/onap/master/appc/oom/kubernetes# kgpa\nNAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE\nkube-system   heapster-76b8cd7b5-l9sdn               1/1       Running   0          10d\nkube-system   kube-dns-5d7b4487c9-j5ctf              3/3       Running   0          10d\nkube-system   kubernetes-dashboard-f9577fffd-9sqfc   1/1       Running   0          10d\nkube-system   monitoring-grafana-997796fcf-krq9l     1/1       Running   0          10d\nkube-system   monitoring-influxdb-56fdcd96b-tvtm2    1/1       Running   0          10d\nkube-system   tiller-deploy-54bcc55dd5-sxcl2         1/1       Running   0          10d\nonap          aaa-appc-0                             2/2       Running   0          5m\nonap          aaa-appc-1                             2/2       Running   0          5m\nonap          aaa-appc-2                             2/2       Running   0          5m\nonap          aaa-appc-cdt-54df796868-vnk4b          1/1       Running   0          5m\nonap          aaa-appc-db-0                          2/2       Running   0          5m\nonap          aaa-appc-dgbuilder-856445d5d-gjgs4     1/1       Running   0          5m\nroot@borislav-new-rancher:~/onap/master/appc/oom/kubernetes# kg pv\nNAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                           STORAGECLASS       REASON    AGE\naaa-appc-data0     1Gi        RWO            Retain           Bound     onap/aaa-appc-data-aaa-appc-2   aaa-appc-data                5m\naaa-appc-data1     1Gi        RWO            Retain           Bound     onap/aaa-appc-data-aaa-appc-0   aaa-appc-data                5m\naaa-appc-data2     1Gi        RWO            Retain           Bound     onap/aaa-appc-data-aaa-appc-1   aaa-appc-data                5m\naaa-appc-db-data   1Gi        RWX            Retain           Bound     onap/aaa-appc-db-data           aaa-appc-db-data             5m\nroot@borislav-new-rancher:~/onap/master/appc/oom/kubernetes# kg pvc -n onap\nNAME                       STATUS    VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS       AGE\naaa-appc-data-aaa-appc-0   Bound     aaa-appc-data1     1Gi        RWO            aaa-appc-data      2h\naaa-appc-data-aaa-appc-1   Bound     aaa-appc-data2     1Gi        RWO            aaa-appc-data      2h\naaa-appc-data-aaa-appc-2   Bound     aaa-appc-data0     1Gi        RWO            aaa-appc-data      2h\naaa-appc-db-data           Bound     aaa-appc-db-data   1Gi        RWX            aaa-appc-db-data   5m\n\nYou can see the PVCs are 2h olds while the PVs are 5m old.",
      "parentUuid": "5a93c517_a4b51b33",
      "range": {
        "startLine": 116,
        "startChar": 1,
        "endLine": 116,
        "endChar": 26
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_3554f2fd",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 117,
      "author": {
        "id": 1733
      },
      "writtenOn": "2018-04-16T12:20:59Z",
      "side": 1,
      "message": "Dave had these comments:\n\nWill that create a unique file instance for each node? Just thinking you could have local shards, in such case you would want each node to have its own instance of the daexim directory.",
      "range": {
        "startLine": 117,
        "startChar": 2,
        "endLine": 117,
        "endChar": 45
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_d56b4eb7",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 117,
      "author": {
        "id": 497
      },
      "writtenOn": "2018-04-16T12:25:24Z",
      "side": 1,
      "message": "There will be a separate persistent volume for each appc instance. Therefore effectively there will be a unique file per instance.\nDidn\u0027t grasp the nodes and local shards in this context. do you mean k8s nodes or appc instances?",
      "parentUuid": "5a93c517_3554f2fd",
      "range": {
        "startLine": 117,
        "startChar": 2,
        "endLine": 117,
        "endChar": 45
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a93c517_b5ad62eb",
        "filename": "kubernetes/appc/values.yaml",
        "patchSetId": 1
      },
      "lineNbr": 117,
      "author": {
        "id": 1733
      },
      "writtenOn": "2018-04-16T12:31:05Z",
      "side": 1,
      "message": "MD-SAL shards can be replicated or local, Dave wanted to make sure if you had a local shard that it was backed up and could be restored. I think we are good here based on your response.",
      "parentUuid": "5a93c517_3554f2fd",
      "range": {
        "startLine": 117,
        "startChar": 2,
        "endLine": 117,
        "endChar": 45
      },
      "revId": "34de73cd92582d6b4dd812e5bdef877f87e24d73",
      "serverId": "14b4e41f-c4e3-4fb9-9955-6a9b5656895a",
      "unresolved": false
    }
  ]
}